{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider cases where we classify input $\\textbf{x}$ in the input space $C(\\mathbf{X})$ to a class $C_k$ and we have a discriminant function (or any map) that maps $\\textbf{x}$ to a class $C_k \\in  \\{C_1, ..., C_K\\}$\n",
    "\n",
    "Evaluation metrics for classification problem:\n",
    "\n",
    " $P_{\\text{true}}:=|\\{\\mathbf{x} \\in C(\\mathbf{X}): y(\\mathbf{x}) = C_k | \\mathbf{x} \\in C_k \\}| $\n",
    "  \n",
    " $P_{\\text{false}}:=|\\{\\mathbf{x} \\in C(\\mathbf{X}): y(\\mathbf{x}) = C_k | \\mathbf{x} \\notin C_k \\}| $\n",
    " \n",
    " $N_{\\text{true}}:=|\\{\\mathbf{x} \\in C(\\mathbf{X}): y(\\mathbf{x}) \\neq C_k | \\mathbf{x} \\notin C_k \\}| $\n",
    "  \n",
    " $N_{\\text{false}}:=|\\{\\mathbf{x} \\in C(\\mathbf{X}): y(\\mathbf{x}) \\neq C_k | \\mathbf{x} \\in C_k \\}| $\n",
    " \n",
    " \n",
    " and we have total counts $\\text{TOTAL} : = P_{\\text{true}} +P_{\\text{false}} + N_{\\text{true}} + N_{\\text{false}} $\n",
    " \n",
    " Accuracy $Acc := \\frac{P_{\\text{true}} + N_{\\text{true}} }{TOTAL}$\n",
    " \n",
    " Precision $Pcs := \\frac{P_{\\text{true}}}{P_{\\text{true}}+P_{\\text{false}} }$\n",
    " \n",
    " Recall $Rec := \\frac{P_{\\text{true}}}{P_{\\text{true}}+F_{\\text{false}} }$\n",
    " \n",
    " F_1 measure $F_1 := \\frac{2*Pcs * Rec}{Pcs+Rec}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA returned me the result:\n",
    "\n",
    "Accuracy would be  0.9575\n",
    "\n",
    "Precision would be  0.9463414634146341\n",
    "\n",
    "Recall would be 0.97\n",
    "\n",
    "F-measure would be 0.9580246913580247"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w = beta = (-14.85992178   8.72562473   5.68227111   3.27898269   9.92186112,\n",
    "            4.555445   -17.12291339  24.46050271  29.71995544  -9.3559337\n",
    "  13.30979447  12.47838008 -15.75210565 -13.07731511   5.72648095\n",
    " -13.23577877 -30.10077125   6.80408844   0.65591131   5.05253155)\n",
    "\n",
    "\n",
    "w_0 = beta_0 = bias = -27.81483255516115"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GDA performed far better than K-NN, since GDA performs almost 40% better accuracy than Nearest neighbor methods.\n",
    "\n",
    "The code suggests that at $K = 2$ we get the highest f1 measure. This change happens since K is hyperparameter, and it seems that the f1 measure shows a periodic pattern.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k: 2\n",
    "Accuracy would be: 0.56625\n",
    "Precision obtained: 0.5459272097053726\n",
    "Recall is: 0.7875\n",
    "F Measure observed: 0.6448311156601843\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model on test set had following measures:\n",
    "Accuracy would be  0.51125\n",
    "Precision would be  0.5115089514066496\n",
    "Recall would be 0.5\n",
    "F-measure would be 0.5056890012642226\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w = [ 0.03536196 -0.01068875 -0.00038642 -0.00916522 -0.06555391 -0.01634108\n",
    "  0.04971012  0.02155161 -0.04725721 -0.01966905 -0.02844503 -0.06206904\n",
    " -0.00700645 -0.04527776  0.07074852 -0.00875106  0.05179928 -0.04166657\n",
    "  0.05683159 -0.00696062]\n",
    "w0 = 0.0932302347357511"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. By a slight margin GDA performs better since GDA model attain the posterior probabilities, which gives significant advantage over non-parametric methods such as K-NN.\n",
    "The K-NN classifier performed the best at K=4. The reason why this plays a role is due to the fact that K is a hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the classifier depended on the difference of two distribution, GDA performed far better than K-NN since GDA uses the distributional properties. However, we could notice that when the distribution and class got less correlated the advantage GDA showed got less pronounced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
