{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Binary Bag of Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Dan Yunheum Seol\n",
    "260677676\n",
    "Collaborated with Aanika Rahman, Ramsha Ijaz\n",
    "Got advice and help from Chlo√© Pierret, Peter Quinn\n",
    "'''\n",
    "# import essential libraries\n",
    "\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator as op\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.dummy import DummyClassifier\n",
    "# ...\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit, ParameterGrid\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# for classifiers\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import ast\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "yelp_tr = pd.read_csv(\"hwk3_datasets/yelp-train.txt\", sep='\\t', lineterminator='\\n', header=None, names=['review', 'label'])\n",
    "yelp_te = pd.read_csv(\"hwk3_datasets/yelp-test.txt\", sep='\\t', lineterminator='\\n', header=None, names=['review', 'label'])\n",
    "yelp_va = pd.read_csv(\"hwk3_datasets/yelp-valid.txt\", sep='\\t', lineterminator='\\n', header=None, names=['review', 'label'])\n",
    "imdb_tr = pd.read_csv(\"hwk3_datasets/IMDB-train.txt\", sep='\\t', lineterminator='\\n', header=None, names=['review', 'label'])\n",
    "imdb_te = pd.read_csv(\"hwk3_datasets/IMDB-test.txt\", sep='\\t', lineterminator='\\n', header=None, names=['review', 'label'])\n",
    "imdb_va = pd.read_csv(\"hwk3_datasets/IMDB-valid.txt\", sep='\\t', lineterminator='\\n', header=None, names=['review', 'label'])\n",
    "# categories of given dataset\n",
    "hw3_datasets = {\n",
    "    'Yelp': {'train': yelp_tr, 'valid': yelp_va, 'test': yelp_te},\n",
    "    'IMDB': {'train': imdb_tr, 'valid': imdb_va, 'test': imdb_te},\n",
    "}\n",
    "#Pre-processing:\n",
    "#You make the sentences to lower case\n",
    "\n",
    "for dataset in hw3_datasets.values():\n",
    "    for df in dataset.values():\n",
    "        df['review'] = df['review'].str.lower()\n",
    "        df['review'] = df['review'].str.replace('<br /><br />', ' ').str.replace('[^\\w\\s]', '')\n",
    "\n",
    "vocab = {}\n",
    "#We exclude the words that do not have much semantic value: such as \"the\"\n",
    "#NLTK's stop words list\n",
    "stops = {'the','a','i','me', 'youre', 'not', 'my', 'myself','we','our','ours','ourselves','you','your','yours','yourself','yourselves','he','him','his','himself','she','her','hers','herself','it','its','itself','they','them','their','theirs','themselves','what','which','who','whom','this','that','these','those','am','is','are','was','were','be','been','being','have','has','had','having','do','does','did','doing','and','but','if','or','because','as','until','while','of','at','by','for','with','about','against','between','into','through','during','before','after','above','below','to','from','up','down','in','out','on','off','over','under','again','further','then','once', 'there','when','where','why','how','all','any','both','each','most','other','some','such','nor','only','so','than','too','very','s','t','can','will','just','don','should','now'}\n",
    "for group_name, group in hw3_datasets.items():\n",
    "    list_all_words = [word for sentence in group['train']['review'].str.split().tolist() for word in sentence]\n",
    "    list_freq_words = Counter(word for word in list_all_words if word not in stops).most_common(10000)\n",
    "    vocab[group_name] = {word[0]: i for i, word in enumerate(list_freq_words)}\n",
    "vtzrIMDB = CountVectorizer(max_features = 10000, binary=True, vocabulary= vocab['IMDB']) #make it onehot encoded\n",
    "train = hw3_datasets['IMDB']['train']\n",
    "test = hw3_datasets['IMDB']['test']\n",
    "val = hw3_datasets['IMDB']['valid']\n",
    "train_vectors = vtzrIMDB.fit_transform(train['review'])\n",
    "test_vectors = vtzrIMDB.transform(test['review'])\n",
    "val_vectors = vtzrIMDB.transform(val['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test methods\n",
    "#methods to get f1 score for training, valid, and test set\n",
    "def acc_csfier(csfier, vec, label):\n",
    "    try: \n",
    "        csfier.fit(train_vectors, train['label'])\n",
    "        yhat = csfier.predict(vec)\n",
    "    except: #classifiers such as GaussianNB cannot take input in sparse matrix form\n",
    "        csfier.fit(train_vectors.toarray(), train['label'])\n",
    "        yhat = csfier.predict(vec.toarray())\n",
    "       \n",
    "    acc_sc = accuracy_score(label,yhat)\n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "    return acc_sc\n",
    "\n",
    "def f1_va_csf(csfier):\n",
    "    try: \n",
    "        csfier.fit(train_vectors, train['label'])  \n",
    "        va_y = csfier.predict(val_vectors)\n",
    "    except: #classifiers such as GaussianNB cannot take input in sparse matrix form\n",
    "        csfier.fit(train_vectors.toarray(), train['label'])\n",
    "        va_y = csfier.predict(val_vectors.toarray())\n",
    "    va_f = f1_score(val['label'], va_y,average='macro')\n",
    "    return va_f\n",
    "def f1_te_csf(csfier):\n",
    "    try:\n",
    "        csfier.fit(train_vectors, train['label'])\n",
    "        te_y = csfier.predict(test_vectors)\n",
    "    except: #classifiers such as GaussianNB cannot take input in sparse matrix form\n",
    "        csfier.fit(train_vectors.toarray(), train['label'])\n",
    "        te_y = csfier.predict(test_vectors.toarray())\n",
    "    te_f = f1_score(test['label'], te_y, average='macro')\n",
    "    return te_f\n",
    "def f1_tr_csf(csfier):\n",
    "    try: \n",
    "        csfier.fit(train_vectors, train['label'])  \n",
    "        tr_y = csfier.predict(train_vectors)\n",
    "    except: #classifiers such as GaussianNB cannot take input in sparse matrix form\n",
    "        csfier.fit(train_vectors.toarray(), train['label'])\n",
    "        tr_y = csfier.predict(train_vectors.toarray())\n",
    "    tr_f = f1_score(train['label'], tr_y,average='macro')\n",
    "    return tr_f\n",
    "    \n",
    "       \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score of Random Classifier on Test: 0.49505468890935816\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random = DummyClassifier(strategy='uniform', random_state=329) #set random seed so we get consistent results\n",
    "\n",
    "print(f\"F1 Score of Random Classifier on Test: {f1_te_csf(random)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_params = ParameterGrid({'alpha':[0.05,.1,0.25, .5, 1, 2, 4]})\n",
    "tree_params = ParameterGrid({'random_state':[329],'criterion':['gini','entropy'],'max_depth':[None,10,100,1000],'min_samples_split':[2,5,10]})\n",
    "svm_params = ParameterGrid({'random_state':[329],'loss':['hinge','squared_hinge'],'C':[0.025, 0.05, 0.1, 0.25,0.5,1,2.0]})\n",
    "\n",
    "classifiers= [(BernoulliNB, bayes_params), (DecisionTreeClassifier, tree_params), (svm.LinearSVC, svm_params)]\n",
    "# find best params for a classifier \n",
    "def tune_hyper(classifier, param_grid):\n",
    "    best_score=0 #f1 score on validation\n",
    "    best_params=None\n",
    "    for params in param_grid:\n",
    "        print(f\"Attempt with : {params}\")\n",
    "        score = f1_va_csf(classifier(**params))\n",
    "        print(f\"F1 Score for the Validation set would be : {score}\\n\")\n",
    "        if score>best_score:\n",
    "            best_score=score\n",
    "            best_params=params       \n",
    "            \n",
    "    print(f\"Optimal parameters for Validation is : {best_params}\")\n",
    "    print(f\"F1 Score on Validation set given optimal parameters is: {best_score}\\n\")\n",
    "  \n",
    "    return classifier(**best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.naive_bayes.BernoulliNB'>\n",
      "Attempt with : {'alpha': 0.05}\n",
      "F1 Score for the Validation set would be : 0.8484363053573265\n",
      "\n",
      "Attempt with : {'alpha': 0.1}\n",
      "F1 Score for the Validation set would be : 0.8486351416718549\n",
      "\n",
      "Attempt with : {'alpha': 0.25}\n",
      "F1 Score for the Validation set would be : 0.8483331149036725\n",
      "\n",
      "Attempt with : {'alpha': 0.5}\n",
      "F1 Score for the Validation set would be : 0.8475275389876284\n",
      "\n",
      "Attempt with : {'alpha': 1}\n",
      "F1 Score for the Validation set would be : 0.846513058857059\n",
      "\n",
      "Attempt with : {'alpha': 2}\n",
      "F1 Score for the Validation set would be : 0.8461836764052815\n",
      "\n",
      "Attempt with : {'alpha': 4}\n",
      "F1 Score for the Validation set would be : 0.8450393305795181\n",
      "\n",
      "Optimal parameters for Validation is : {'alpha': 0.1}\n",
      "F1 Score on Validation set given optimal parameters is: 0.8486351416718549\n",
      "\n",
      "Here is our f1 score for the test set with optimal parameters: 0.8353844574163147\n",
      "\n",
      "Here is our f1 score for the training set given optimal parameters: 0.8757402222106325\n",
      "\n",
      "<class 'sklearn.tree.tree.DecisionTreeClassifier'>\n",
      "Attempt with : {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.6942980893630585\n",
      "\n",
      "Attempt with : {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.6916948173898803\n",
      "\n",
      "Attempt with : {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.6925972333751005\n",
      "\n",
      "Attempt with : {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.7110771570362202\n",
      "\n",
      "Attempt with : {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.7107543842351427\n",
      "\n",
      "Attempt with : {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.7097490119175729\n",
      "\n",
      "Attempt with : {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.6905821280237147\n",
      "\n",
      "Attempt with : {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.6920606792281306\n",
      "\n",
      "Attempt with : {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.692598512176799\n",
      "\n",
      "Attempt with : {'criterion': 'gini', 'max_depth': 1000, 'min_samples_split': 2, 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.6942980893630585\n",
      "\n",
      "Attempt with : {'criterion': 'gini', 'max_depth': 1000, 'min_samples_split': 5, 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.6916948173898803\n",
      "\n",
      "Attempt with : {'criterion': 'gini', 'max_depth': 1000, 'min_samples_split': 10, 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.6925972333751005\n",
      "\n",
      "Attempt with : {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.7025769675603679\n",
      "\n",
      "Attempt with : {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.7060989390171699\n",
      "\n",
      "Attempt with : {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.7021878016123542\n",
      "\n",
      "Attempt with : {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.7081859156374325\n",
      "\n",
      "Attempt with : {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.7068901822027767\n",
      "\n",
      "Attempt with : {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.7068721407582905\n",
      "\n",
      "Attempt with : {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.7056711528296888\n",
      "\n",
      "Attempt with : {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.7052554080906901\n",
      "\n",
      "Attempt with : {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.7083460556864598\n",
      "\n",
      "Attempt with : {'criterion': 'entropy', 'max_depth': 1000, 'min_samples_split': 2, 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.7025769675603679\n",
      "\n",
      "Attempt with : {'criterion': 'entropy', 'max_depth': 1000, 'min_samples_split': 5, 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.7060989390171699\n",
      "\n",
      "Attempt with : {'criterion': 'entropy', 'max_depth': 1000, 'min_samples_split': 10, 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.7021878016123542\n",
      "\n",
      "Optimal parameters for Validation is : {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'random_state': 329}\n",
      "F1 Score on Validation set given optimal parameters is: 0.7110771570362202\n",
      "\n",
      "Here is our f1 score for the test set with optimal parameters: 0.7101659983178783\n",
      "\n",
      "Here is our f1 score for the training set given optimal parameters: 0.758296532392636\n",
      "\n",
      "<class 'sklearn.svm.classes.LinearSVC'>\n",
      "Attempt with : {'C': 0.025, 'loss': 'hinge', 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.8751889716975392\n",
      "\n",
      "Attempt with : {'C': 0.025, 'loss': 'squared_hinge', 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.8718969242451511\n",
      "\n",
      "Attempt with : {'C': 0.05, 'loss': 'hinge', 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.8678977793616711\n",
      "\n",
      "Attempt with : {'C': 0.05, 'loss': 'squared_hinge', 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.865498868845487\n",
      "\n",
      "Attempt with : {'C': 0.1, 'loss': 'hinge', 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.8622999655749914\n",
      "\n",
      "Attempt with : {'C': 0.1, 'loss': 'squared_hinge', 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.8619998012797139\n",
      "\n",
      "Attempt with : {'C': 0.25, 'loss': 'hinge', 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.8546998241867874\n",
      "\n",
      "Attempt with : {'C': 0.25, 'loss': 'squared_hinge', 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.8530998222507848\n",
      "\n",
      "Attempt with : {'C': 0.5, 'loss': 'hinge', 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.8474994494730127\n",
      "\n",
      "Attempt with : {'C': 0.5, 'loss': 'squared_hinge', 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.8498998784189016\n",
      "\n",
      "Attempt with : {'C': 1, 'loss': 'hinge', 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.8419992352762988\n",
      "\n",
      "Attempt with : {'C': 1, 'loss': 'squared_hinge', 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.8460996537242209\n",
      "\n",
      "Attempt with : {'C': 2.0, 'loss': 'hinge', 'random_state': 329}\n",
      "F1 Score for the Validation set would be : 0.840699424924924\n",
      "\n",
      "Attempt with : {'C': 2.0, 'loss': 'squared_hinge', 'random_state': 329}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cayman329/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for the Validation set would be : 0.8419990899147579\n",
      "\n",
      "Optimal parameters for Validation is : {'C': 0.025, 'loss': 'hinge', 'random_state': 329}\n",
      "F1 Score on Validation set given optimal parameters is: 0.8751889716975392\n",
      "\n",
      "Here is our f1 score for the test set with optimal parameters: 0.8697556788060092\n",
      "\n",
      "Here is our f1 score for the training set given optimal parameters: 0.9528659860515052\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for pair in classifiers: # cycle through the classifiers and parameters\n",
    "    classifier = pair[0]\n",
    "    param_grid = pair[1]\n",
    "    print(classifier)    \n",
    "    best_classifier = tune_hyper(classifier,param_grid) \n",
    "    print(f\"Here is our f1 score for the test set with optimal parameters: {f1_te_csf(best_classifier)}\\n\")\n",
    "    print(f\"Here is our f1 score for the training set given optimal parameters: {f1_tr_csf(best_classifier)}\\n\")\n",
    "    #print(f\"Here is our accuracy score for the validation set with optimal parameters: {acc_csfier(best_classifier, val_vectors, val['label'])}\\n\")    \n",
    "    #print(f\"Here is our accuracy score for the validation set with optimal parameters: {acc_csfier(best_classifier, test_vectors, test['label'])}\\n\")      \n",
    "    #print(f\"Here is our accuracy score for the validation set with optimal parameters: {acc_csfier(best_classifier, train_vectors, train['label'])}\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
