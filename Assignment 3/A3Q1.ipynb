{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import essential libraries\n",
    "\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator as op\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "# ...\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.metrics import f1_score\n",
    "# for classifiers\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import ast\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp_tr = pd.read_csv(\"hwk3_datasets/yelp-train.txt\", sep='\\t', lineterminator='\\n', header=None, names=['review', 'label'])\n",
    "yelp_te = pd.read_csv(\"hwk3_datasets/yelp-test.txt\", sep='\\t', lineterminator='\\n', header=None, names=['review', 'label'])\n",
    "yelp_va = pd.read_csv(\"hwk3_datasets/yelp-valid.txt\", sep='\\t', lineterminator='\\n', header=None, names=['review', 'label'])\n",
    "imdb_tr = pd.read_csv(\"hwk3_datasets/IMDB-train.txt\", sep='\\t', lineterminator='\\n', header=None, names=['review', 'label'])\n",
    "imdb_te = pd.read_csv(\"hwk3_datasets/IMDB-test.txt\", sep='\\t', lineterminator='\\n', header=None, names=['review', 'label'])\n",
    "imdb_va = pd.read_csv(\"hwk3_datasets/IMDB-valid.txt\", sep='\\t', lineterminator='\\n', header=None, names=['review', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# categories of given dataset\n",
    "hw3_datasets = {\n",
    "    'Yelp': {'train': yelp_tr, 'valid': yelp_va, 'test': yelp_te},\n",
    "    'IMDB': {'train': imdb_tr, 'valid': imdb_va, 'test': imdb_te},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pre-processing:\n",
    "#You make the sentences to lower case\n",
    "\n",
    "for dataset in hw3_datasets.values():\n",
    "    for df in dataset.values():\n",
    "        df['review'] = df['review'].str.lower()\n",
    "        df['review'] = df['review'].str.replace('<br /><br />', ' ').str.replace('[^\\w\\s]', '')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yelp train  is of size: 7000\n",
      "Yelp valid  is of size: 1000\n",
      "Yelp test  is of size: 2000\n",
      "IMDB train  is of size: 15000\n",
      "IMDB valid  is of size: 10000\n",
      "IMDB test  is of size: 25000\n",
      "Yelp train:\n",
      "                                              review  label\n",
      "0  i cant believe i havent yelped about the place...      5\n",
      "1  best nights to go to postinos are mondays and ...      5\n",
      "2  went here tonight with the padres and husband ...      5\n",
      "3  i must be spoiled and realize that this is not...      3\n",
      "4  normally love this store  have been a member f...      2 \n",
      "\n",
      "IMDB train:\n",
      "                                              review  label\n",
      "0  for a movie that gets no respect there sure ar...      1\n",
      "1  bizarre horror movie filled with famous faces ...      1\n",
      "2  a solid if unremarkable film matthau as einste...      1\n",
      "3  its a strange feeling to sit alone in a theate...      1\n",
      "4  you probably all already know this by now but ...      1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check dataset sizes\n",
    "for d_n, d in hw3_datasets.items():\n",
    "    for s_n, s in d.items():\n",
    "        print(d_n, s_n, ' is of size:', s.shape[0])\n",
    "# check dataset contents\n",
    "for d_n, d in hw3_datasets.items():\n",
    "    print(d_n, 'train:')\n",
    "    print(d['train'].head(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "#We exclude the words that do not have much semantic value: such as \"the\"\n",
    "#NLTK's stop words list\n",
    "stops = {'the','a','i','me', 'youre', 'not', 'my', 'myself','we','our','ours','ourselves','you','your','yours','yourself','yourselves','he','him','his','himself','she','her','hers','herself','it','its','itself','they','them','their','theirs','themselves','what','which','who','whom','this','that','these','those','am','is','are','was','were','be','been','being','have','has','had','having','do','does','did','doing','and','but','if','or','because','as','until','while','of','at','by','for','with','about','against','between','into','through','during','before','after','above','below','to','from','up','down','in','out','on','off','over','under','again','further','then','once', 'there','when','where','why','how','all','any','both','each','most','other','some','such','nor','only','so','than','too','very','s','t','can','will','just','don','should','now', 'an', 'They', 'So'}   \n",
    "for group_name, group in hw3_datasets.items():\n",
    "    list_all_words = [word for sentence in group['train']['review'].str.split().tolist() for word in sentence]\n",
    "    list_freq_words = Counter(word for word in list_all_words if word not in stops).most_common(10000)\n",
    "    vocab[group_name] = {word[0]: i for i, word in enumerate(list_freq_words)}\n",
    "    # save \"-vocab.txt\" file\n",
    "    if 1:\n",
    "        file_vocab = pd.DataFrame(list_freq_words)\n",
    "        file_vocab[2] = np.arange(0, 10000) # word IDs\n",
    "        file_vocab.to_csv('./hwk3_datasets/submission/' + group_name + '-vocab.txt', sep='\\t', header=False, index=False, columns=[0, 2, 1])\n",
    "    # save \"-train.txt\", \"-valid.txt\", \"-test.txt\" file\n",
    "    if 1:\n",
    "        for dataset_name, dataset in group.items():\n",
    "            with open('./hwk3_datasets/submission/' + group_name + '-' + dataset_name + '.txt', 'w') as file:\n",
    "                for i in range(len(dataset)):\n",
    "                    file.write(' '.join([str(vocab[group_name][word]) for word in dataset.iloc[i, 0].split() if word in vocab[group_name]]) \n",
    "                               + '\\t' + str(dataset.iloc[i, 1]) + '\\n')   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
