{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.naive_bayes.GaussianNB'>\n",
      "Trying: {}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "fit() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-19450a7be716>\u001b[0m in \u001b[0;36meval_csfier_quick\u001b[0;34m(classifier)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mcsfier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mval_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsfier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() missing 1 required positional argument: 'y'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-19450a7be716>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mbest_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimal_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test sc for best params: {test_classifier(best_classifier)}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train sc for best params: {test_csfier_train(best_classifier)}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-19450a7be716>\u001b[0m in \u001b[0;36moptimal_parameters\u001b[0;34m(classifier, param_grid)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Trying: {params}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_csfier_quick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"F1 sc Validation: {sc}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mopt_sc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-19450a7be716>\u001b[0m in \u001b[0;36meval_csfier_quick\u001b[0;34m(classifier)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mval_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsfier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#some of the classifiers can't deal with sparse matrices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mcsfier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mval_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsfier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "'''Dan Yunheum Seol\n",
    "260677676\n",
    "Collaborated with Aanika Rahman, Ramsha Ijaz\n",
    "Got advice and help from Chlo√© Pierret, Peter Quinn\n",
    "'''\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# # Yelp Frequency Bag-of-Words\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "# import essential libraries\n",
    "\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator as op\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score #f1_score(y_true, y_pred)\n",
    "\n",
    "# ...\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit, ParameterGrid\n",
    "from sklearn.metrics import f1_score\n",
    "# for csfiers\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# examples are split with  \\n\n",
    "# rating given with review is last char in example\n",
    "yelp_tr = pd.read_csv(\"yelp-train.txt\", sep='\\t', lineterminator='\\n', header=None, names=['review', 'label'])\n",
    "yelp_te = pd.read_csv(\"yelp-test.txt\", sep='\\t', lineterminator='\\n', header=None, names=['review', 'label'])\n",
    "yelp_va = pd.read_csv(\"yelp-valid.txt\", sep='\\t', lineterminator='\\n', header=None, names=['review', 'label'])\n",
    "imdb_tr = pd.read_csv(\"IMDB-train.txt\", sep='\\t', lineterminator='\\n', header=None, names=['review', 'label'])\n",
    "imdb_te = pd.read_csv(\"IMDB-test.txt\", sep='\\t', lineterminator='\\n', header=None, names=['review', 'label'])\n",
    "imdb_va = pd.read_csv(\"IMDB-valid.txt\", sep='\\t', lineterminator='\\n', header=None, names=['review', 'label'])\n",
    "\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# categories of given dataset\n",
    "hw3_datasets = {\n",
    "    'Yelp': {'train': yelp_tr, 'valid': yelp_va, 'test': yelp_te},\n",
    "    'IMDB': {'train': imdb_tr, 'valid': imdb_va, 'test': imdb_te},\n",
    "}\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "#Pre-processing:\n",
    "#You make the sentences to lower case\n",
    "\n",
    "for dataset in hw3_datasets.values():\n",
    "    for df in dataset.values():\n",
    "        df['review'] = df['review'].str.lower()\n",
    "        df['review'] = df['review'].str.replace('<br /><br />', ' ').str.replace('[^\\w\\s]', '')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "vocab = {}\n",
    "#We exclude the words that do not have much semantic value: such as \"the\"\n",
    "#NLTK's stop words list\n",
    "stops = {'the','a','i','me', 'youre', 'not', 'my', 'myself','we','our','ours','ourselves','you','your','yours','yourself','yourselves','he','him','his','himself','she','her','hers','herself','it','its','itself','they','them','their','theirs','themselves','what','which','who','whom','this','that','these','those','am','is','are','was','were','be','been','being','have','has','had','having','do','does','did','doing','and','but','if','or','because','as','until','while','of','at','by','for','with','about','against','between','into','through','during','before','after','above','below','to','from','up','down','in','out','on','off','over','under','again','further','then','once', 'there','when','where','why','how','all','any','both','each','most','other','some','such','nor','only','so','than','too','very','s','t','can','will','just','don','should','now'}\n",
    "for group_name, group in hw3_datasets.items():\n",
    "    list_all_words = [word for sentence in group['train']['review'].str.split().tolist() for word in sentence]\n",
    "    list_freq_words = Counter(word for word in list_all_words if word not in stops).most_common(10000)\n",
    "    vocab[group_name] = {word[0]: i for i, word in enumerate(list_freq_words)}\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "vtzrYelp = CountVectorizer(max_features = 10000, binary=False, vocabulary= vocab['Yelp']) #make it onehot encoded\n",
    "train = hw3_datasets['Yelp']['train']\n",
    "test = hw3_datasets['Yelp']['test']\n",
    "val = hw3_datasets['Yelp']['valid']\n",
    "train_vectors = vtzrYelp.fit_transform(train['review'])\n",
    "test_vectors = vtzrYelp.transform(test['review'])\n",
    "val_vectors = vtzrYelp.transform(val['review'])\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "#print(train_vectors)\n",
    "type(train_vectors) #what type does it return?\n",
    "X = train_vectors.toarray() #change it to a 2d array\n",
    "#print(sp.sparse.csr_matrix(train_vectors.toarray())) #could I change it back to sparse.csr_matrix?\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "#Yelp_vocab_stored = pd.read_csv(\"/submission/Yelp-vocab.txt\", sep='\\t', lineterminator='\\n', header=None, names=['word', 'index', 'count'])\n",
    "#Yelp_counts = Yelp_vocab_stored['count'] # get the count column so that you can divide each entry of the vector\n",
    "normalizer = Normalizer(norm='l1')\n",
    "\n",
    "train_vectors_norm = normalizer.transform(train_vectors)\n",
    "test_vectors_norm = normalizer.transform(test_vectors)\n",
    "val_vectors_norm = normalizer.transform(val_vectors)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "bayes_params = ParameterGrid({}) #We have no hyperparameters to adjust..yet\n",
    "tree_params = ParameterGrid({'random_state':[329],'criterion':['gini','entropy'],'max_depth':[None,10,100,1000],'min_samples_split':[2,5,10]})\n",
    "svm_params = ParameterGrid({'random_state':[329],'loss':['hinge','squared_hinge'],'C':[1.0,.5,2.0,5.0]})\n",
    "\n",
    "classifiers= [(GaussianNB, bayes_params), (DecisionTreeClassifier, tree_params), (svm.LinearSVC, svm_params)]\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "def eval_csfier_quick(classifier):\n",
    "    try:\n",
    "        csfier.fit(train_vectors, train['label'])\n",
    "        val_y = csfier.predict(val_vectors)\n",
    "    except: #some of the classifiers can't deal with sparse matrices\n",
    "        csfier.fit(train_vectors.toarray(), train['label'])\n",
    "        val_y = csfier.predict(val_vectors.toarray())\n",
    "\n",
    "    val_f1 = f1_sc(val['label'],val_y, average='macro')\n",
    "    return val_f1\n",
    "\n",
    "def test_classifier(classifier):\n",
    "    try:\n",
    "        csfier.fit(train_vectors, train['label'])\n",
    "        test_y = csfier.predict(test_vectors)\n",
    "    except: #some of the classifiers can't deal with sparse matrices\n",
    "        csfier.fit(train_vectors.toarray(), train['label'])\n",
    "        test_y = csfier.predict(test_vectors.toarray())\n",
    "\n",
    "    test_f1 = f1_sc(test['label'],test_y, average='macro')\n",
    "    return test_f1\n",
    "\n",
    "def test_csfier_train(classifier):\n",
    "    try:\n",
    "        csfier.fit(train_vectors, train['label'])\n",
    "        train_y = csfier.predict(train_vectors)\n",
    "    except: #some of the classifiers can't deal with sparse matrices\n",
    "        csfier.fit(train_vectors.toarray(), train['label'])\n",
    "        train_y = csfier.predict(train_vectors.toarray())\n",
    "\n",
    "    train_f1 = f1_sc(train['label'], train_y, average='macro')\n",
    "    return train_f1\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "# find best params for a classifier\n",
    "def optimal_parameters(classifier, param_grid):\n",
    "    opt_sc=0 #f1 sc on validation\n",
    "    best_params=None\n",
    "    for params in param_grid:\n",
    "        print(f\"Trying: {params}\")\n",
    "        sc = eval_csfier_quick(classifier(**params))\n",
    "        print(f\"F1 sc Validation: {sc}\\n\")\n",
    "        if sc>opt_sc:\n",
    "            opt_sc=sc\n",
    "            best_params=params\n",
    "\n",
    "    print(f\"Best params for Validation: {best_params}\")\n",
    "    print(f\"Best F1 sc on Validation: {opt_sc}\\n\")\n",
    "\n",
    "    return classifier(**best_params)\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "def optimal_GaussianNB(vec_smoothing):\n",
    "    opt_sc = 0\n",
    "    best_params = -1\n",
    "    for i in range(len(vec_smoothing)):\n",
    "        print(f\"Trying:{vec_smoothing[i]}\")\n",
    "        csfier = GaussianNB(priors=None, var_smoothing=vec_smoothing[i])\n",
    "        csfier.fit(train_vectors.toarray(), train['label'])\n",
    "        val_y = csfier.predict(val_vectors.toarray())\n",
    "        val_f1 = f1_score(val['label'],val_y, average='macro')\n",
    "        print(f\"F1 sc Validation: {val_f1}\\n\")\n",
    "        if  val_f1>opt_sc:\n",
    "            opt_sc= val_f1\n",
    "            best_params=vec_smoothing[i]\n",
    "    print(f\"Best params for Validation: {best_params}\")\n",
    "    print(f\"Best F1 sc on Validation: {opt_sc}\\n\")\n",
    "    return best_params\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "for pair in classifiers: # cycle through the classifiers and parameters\n",
    "    classifier = pair[0]\n",
    "    param_grid = pair[1]\n",
    "    print(classifier)\n",
    "    best_classifier = optimal_parameters(classifier,param_grid)\n",
    "    print(f\"Test sc for best params: {test_classifier(best_classifier)}\\n\")\n",
    "    print(f\"Train sc for best params: {test_csfier_train(best_classifier)}\\n\")\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "#tuning Var_smoothing for GaussianNB\n",
    "vec_smt = [1e-5,1e-6,1e-7, 1e-8, 1e-9, 1e-10, 1e-11,1e-12]\n",
    "best_GNB = optimal_GaussianNB(vec_smt)\n",
    "GNB = GaussianNB(priors=None, var_smoothing = best_GNB)\n",
    "GNB.fit(train_vectors.toarray(), train['label'])\n",
    "GNB_hat = GNB.predict(test_vectors.toarray())\n",
    "GNB_f1_te = f1_score(test['label'],GNB_hat, average='macro')\n",
    "GNB_hat_tr = GNB.predict(train_vectors.toarray())\n",
    "GNB_f1_tr = f1_score(train['label'],GNB_hat_tr, average='macro')\n",
    "print(f\"Test sc for best params: {GNB_f1_te}\\n\")\n",
    "print(f\"Train sc for best params: {GNB_f1_tr}\\n\")\n",
    "\n",
    "\n",
    "# In[30]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
